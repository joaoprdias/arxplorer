{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joaop\\OneDrive\\Documentos\\arxplorer\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_ollama import OllamaLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\joaop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções\n",
    "\n",
    "def get_arxiv_data(query, max_results=250, sort_by=\"relevance\"):\n",
    "    \"\"\"\n",
    "    Obtém artigos da arXiv API com base numa query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Palavra-chave ou expressão de pesquisa.\n",
    "        max_results (int): Número máximo de resultados a retornar\n",
    "        sort_by (str): Critério de ordenação ('relevance', 'submittedDate', 'lastUpdatedDate').\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Dados dos artigos recolhidos.\n",
    "    \"\"\"\n",
    "    base_url = \"http://export.arxiv.org/api/query\"\n",
    "    results = []\n",
    "\n",
    "    # Dividir as pesquisas em lotes de 1000 (limitação da arXiv API)\n",
    "    for start in range(0, max_results, 1000):\n",
    "        params = {\n",
    "            \"search_query\": query,\n",
    "            \"start\": start,\n",
    "            \"max_results\": min(max_results - start, 1000),\n",
    "            \"sortBy\": sort_by,  # Critério de ordenação\n",
    "        }\n",
    "\n",
    "        # Requisição à arXiv API\n",
    "        response = requests.get(base_url, params=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            # Parsear o XML retornado\n",
    "            root = ET.fromstring(response.text)\n",
    "            for entry in root.findall(\"{http://www.w3.org/2005/Atom}entry\"):\n",
    "                # Extrair campos importantes do artigo\n",
    "                title = entry.find(\"{http://www.w3.org/2005/Atom}title\").text.strip()\n",
    "                summary = entry.find(\"{http://www.w3.org/2005/Atom}summary\").text.strip()\n",
    "                link = entry.find(\"{http://www.w3.org/2005/Atom}id\").text.strip()\n",
    "                published = entry.find(\"{http://www.w3.org/2005/Atom}published\").text.strip()\n",
    "                updated = entry.find(\"{http://www.w3.org/2005/Atom}updated\").text.strip()\n",
    "                authors = [\n",
    "                    author.find(\"{http://www.w3.org/2005/Atom}name\").text.strip()\n",
    "                    for author in entry.findall(\"{http://www.w3.org/2005/Atom}author\")\n",
    "                ]\n",
    "\n",
    "                primary_category = entry.find(\"{http://arxiv.org/schemas/atom}primary_category\")\n",
    "                journal_ref = entry.find(\"{http://arxiv.org/schemas/atom}journal_ref\")\n",
    "                comment = entry.find(\"{http://arxiv.org/schemas/atom}comment\")\n",
    "\n",
    "                # Adicionar aos resultados\n",
    "                results.append({\n",
    "                    \"Title\": title,\n",
    "                    \"Summary\": summary,\n",
    "                    \"Authors\": \", \".join(authors),\n",
    "                    \"Link\": link,\n",
    "                    \"Published\": published,\n",
    "                    \"Updated\": updated,\n",
    "                    \"Primary_Category\": primary_category.attrib[\"term\"] if primary_category is not None else None,\n",
    "                    \"Journal_Reference\": journal_ref.text.strip() if journal_ref is not None else None,\n",
    "                    \"Comment\": comment.text.strip() if comment is not None else None,\n",
    "                })\n",
    "        else:\n",
    "            print(f\"Erro na API: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "    # Converter para DataFrame\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def extract_keywords(df, top_n=10):\n",
    "    \"\"\"\n",
    "    Gera três DataFrames separados para palavras individuais, bigramas e trigramas mais frequentes.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): DataFrame com os artigos.\n",
    "        top_n (int): Número de palavras-chave mais frequentes a retornar para cada n-grama.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: DataFrames para unigramas, bigramas e trigramas.\n",
    "    \"\"\"\n",
    "    # Concatenar títulos e resumos em um único texto\n",
    "    combined_text = \" \".join(df[\"Title\"]) + \" \" + \" \".join(df[\"Summary\"])\n",
    "    \n",
    "    # Remover stopwords\n",
    "    stop_words = list(stopwords.words(\"english\"))\n",
    "\n",
    "    # Criar vetorizadores para unigramas, bigramas e trigramas\n",
    "    vectorizer_uni = CountVectorizer(ngram_range=(1, 1), stop_words=stop_words)\n",
    "    vectorizer_bi = CountVectorizer(ngram_range=(2, 2), stop_words=stop_words)\n",
    "    vectorizer_tri = CountVectorizer(ngram_range=(3, 3), stop_words=stop_words)\n",
    "    \n",
    "    # Criar vetores de palavras\n",
    "    word_counts_uni = vectorizer_uni.fit_transform([combined_text])\n",
    "    word_counts_bi = vectorizer_bi.fit_transform([combined_text])\n",
    "    word_counts_tri = vectorizer_tri.fit_transform([combined_text])\n",
    "    \n",
    "    # Criar DataFrames para unigramas\n",
    "    word_list_uni = vectorizer_uni.get_feature_names_out()\n",
    "    count_list_uni = word_counts_uni.toarray().flatten()\n",
    "    unigrams_df = pd.DataFrame({\"Keyword\": word_list_uni, \"Count\": count_list_uni})\n",
    "    unigrams_df = unigrams_df.sort_values(by=\"Count\", ascending=False).head(top_n)\n",
    "    \n",
    "    # Criar DataFrames para bigramas\n",
    "    word_list_bi = vectorizer_bi.get_feature_names_out()\n",
    "    count_list_bi = word_counts_bi.toarray().flatten()\n",
    "    bigrams_df = pd.DataFrame({\"Keyword\": word_list_bi, \"Count\": count_list_bi})\n",
    "    bigrams_df = bigrams_df.sort_values(by=\"Count\", ascending=False).head(top_n)\n",
    "    \n",
    "    # Criar DataFrames para trigramas\n",
    "    word_list_tri = vectorizer_tri.get_feature_names_out()\n",
    "    count_list_tri = word_counts_tri.toarray().flatten()\n",
    "    trigrams_df = pd.DataFrame({\"Keyword\": word_list_tri, \"Count\": count_list_tri})\n",
    "    trigrams_df = trigrams_df.sort_values(by=\"Count\", ascending=False).head(top_n)\n",
    "    \n",
    "    return unigrams_df, bigrams_df, trigrams_df\n",
    "\n",
    "\n",
    "def filter_articles_by_keywords(df, keywords):\n",
    "    \"\"\"\n",
    "    Filtra artigos que contêm palavras-chave relevantes no título ou resumo. \n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame com os artigos.\n",
    "        keywords (list): Lista de palavras-chave relevantes (tem de estar tudo em lower-case).\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Apenas os artigos relevantes.\n",
    "    \"\"\"\n",
    "    def is_relevant(row):\n",
    "        text = (row[\"Title\"] + \" \" + row[\"Summary\"]).lower()\n",
    "        return any(keyword in text for keyword in keywords)\n",
    "    \n",
    "    df[\"Is_Relevant\"] = df.apply(is_relevant, axis=1)\n",
    "    return df[df[\"Is_Relevant\"]]\n",
    "\n",
    "\n",
    "def analyze_publication_trends(df):\n",
    "    \"\"\"\n",
    "    Analisa a frequência de publicações ao longo do tempo.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame com os artigos.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Frequência de publicações por ano.\n",
    "    \"\"\"\n",
    "    df[\"Published_Year\"] = pd.to_datetime(df[\"Published\"]).dt.year\n",
    "    trends = df.groupby(\"Published_Year\").size().reset_index(name=\"Publication_Count\")\n",
    "    return trends\n",
    "\n",
    "\n",
    "def top_authors(df, top_n=10):\n",
    "    \"\"\"\n",
    "    Identifica os autores com mais publicações.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame com os artigos.\n",
    "        top_n (int): Número de autores a listar.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Autores mais frequentes e o número de publicações.\n",
    "    \"\"\"\n",
    "    authors_series = df[\"Authors\"].str.split(\", \").explode()\n",
    "    author_counts = authors_series.value_counts().head(top_n).reset_index()\n",
    "    author_counts.columns = [\"Author\", \"Publication_Count\"]\n",
    "    return author_counts\n",
    "\n",
    "\n",
    "def cluster_articles(df, n_clusters=5):\n",
    "    \"\"\"\n",
    "    Agrupa os artigos em clusters temáticos.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame com os artigos.\n",
    "        n_clusters (int): Número de clusters.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame com uma nova coluna \"Cluster\".\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[\"Summary\"])\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    df[\"Cluster\"] = kmeans.fit_predict(tfidf_matrix)\n",
    "    return df\n",
    "\n",
    "\n",
    "def find_similar_articles_with_url(df, article_url, top_n=5):\n",
    "    \"\"\"\n",
    "    Identifica os artigos mais similares a um artigo específico com base na URL e mostra as URLs nos resultados.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame com os artigos.\n",
    "        article_url (str): URL do artigo para o qual encontrar similares.\n",
    "        top_n (int): Número de artigos similares a retornar.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame com os artigos similares, suas pontuações de similaridade e URLs.\n",
    "    \"\"\"\n",
    "    # Verificar se a URL existe no DataFrame\n",
    "    if article_url not in df[\"Link\"].values:\n",
    "        raise ValueError(\"A URL fornecida não está no conjunto de dados.\")\n",
    "    \n",
    "    # Obter o índice do artigo de interesse\n",
    "    article_index = df[df[\"Link\"] == article_url].index[0]\n",
    "\n",
    "    # Vetorização TF-IDF\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[\"Summary\"])\n",
    "    \n",
    "    # Similaridade do artigo selecionado com todos os outros\n",
    "    similarity_scores = cosine_similarity(tfidf_matrix[article_index], tfidf_matrix).flatten()\n",
    "    \n",
    "    # Ordenar pelos artigos mais similares (excluindo o próprio)\n",
    "    similar_indices = similarity_scores.argsort()[-top_n-1:-1][::-1]\n",
    "    similar_scores = similarity_scores[similar_indices]\n",
    "    \n",
    "    # Criar DataFrame com resultados\n",
    "    similar_articles = df.iloc[similar_indices].copy()\n",
    "    similar_articles[\"Similarity_Score\"] = similar_scores\n",
    "    return similar_articles[[\"Title\", \"Similarity_Score\", \"Link\"]]\n",
    "\n",
    "\n",
    "def generate_summary_ollama(df, n=3):\n",
    "    \"\"\"\n",
    "    Generates a concise summary from the first N summaries using Ollama (Llama3).\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame containing articles (from get_arxiv_data).\n",
    "        n (int): Number of summaries to include in the generated summary.\n",
    "\n",
    "    Returns:\n",
    "        str: Generated summary.\n",
    "    \"\"\"\n",
    "    # Get the first N summaries\n",
    "    summaries = df[\"Summary\"].head(n).tolist()\n",
    "\n",
    "    # Construct the summaries in a formatted list\n",
    "    formatted_summaries = \"\\n\\n\".join([f\"{i+1}. {summary}\" for i, summary in enumerate(summaries)])\n",
    "\n",
    "    # Create the prompt separately\n",
    "    prompt = (\n",
    "        f\"Below are {n} scientific article summaries extracted from arXiv:\\n\\n\"\n",
    "        f\"{formatted_summaries}\\n\\n\"\n",
    "        \"Please generate a concise and coherent summary that combines the key ideas from these summaries.\"\n",
    "    )\n",
    "\n",
    "    # Initialize the Ollama LLM model\n",
    "    model = OllamaLLM(model=\"llama3\")\n",
    "\n",
    "    # Invoke the model with the prompt\n",
    "    result = model.invoke(prompt)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"text mining\"  # Palavra-chave para pesquisas\n",
    "max_results = 250  # Quantidade máxima de artigos\n",
    "\n",
    "articles = get_arxiv_data(query, max_results, sort_by='relevance')\n",
    "unigrams, bigrams, trigrams = extract_keywords(articles, top_n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Link</th>\n",
       "      <th>Published</th>\n",
       "      <th>Updated</th>\n",
       "      <th>Primary_Category</th>\n",
       "      <th>Journal_Reference</th>\n",
       "      <th>Comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data, text and web mining for business intelli...</td>\n",
       "      <td>The Information and Communication Technologies...</td>\n",
       "      <td>Abdul-Aziz Rashid Al-Azmi</td>\n",
       "      <td>http://arxiv.org/abs/1304.3563v1</td>\n",
       "      <td>2013-04-12T08:04:31Z</td>\n",
       "      <td>2013-04-12T08:04:31Z</td>\n",
       "      <td>cs.IR</td>\n",
       "      <td>International Journal of Data Mining &amp; Knowled...</td>\n",
       "      <td>21 page, journal paper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Text Data Mining from the Author's Perspective...</td>\n",
       "      <td>Given the many technical, social, and policy s...</td>\n",
       "      <td>Christine L. Borgman</td>\n",
       "      <td>http://arxiv.org/abs/1803.04552v1</td>\n",
       "      <td>2018-03-12T22:11:40Z</td>\n",
       "      <td>2018-03-12T22:11:40Z</td>\n",
       "      <td>cs.DL</td>\n",
       "      <td>None</td>\n",
       "      <td>Forum Statement: Data Mining with Limited Acce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Very Large Language Model as a Unified Methodo...</td>\n",
       "      <td>Text data mining is the process of deriving es...</td>\n",
       "      <td>Meng Jiang</td>\n",
       "      <td>http://arxiv.org/abs/2212.09271v2</td>\n",
       "      <td>2022-12-19T06:52:13Z</td>\n",
       "      <td>2022-12-20T17:03:30Z</td>\n",
       "      <td>cs.DB</td>\n",
       "      <td>None</td>\n",
       "      <td>4 pages, 3 figures</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pbm: A new dataset for blog mining</td>\n",
       "      <td>Text mining is becoming vital as Web 2.0 offer...</td>\n",
       "      <td>Mehwish Aziz, Muhammad Rafi</td>\n",
       "      <td>http://arxiv.org/abs/1201.2073v1</td>\n",
       "      <td>2012-01-10T15:18:38Z</td>\n",
       "      <td>2012-01-10T15:18:38Z</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>None</td>\n",
       "      <td>6; Internet and Web Engineering from: Internat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Typesafe Modeling in Text Mining</td>\n",
       "      <td>Based on the concept of annotation-based agent...</td>\n",
       "      <td>Fabian Steeg</td>\n",
       "      <td>http://arxiv.org/abs/1108.0363v1</td>\n",
       "      <td>2011-07-28T17:46:20Z</td>\n",
       "      <td>2011-07-28T17:46:20Z</td>\n",
       "      <td>cs.PL</td>\n",
       "      <td>None</td>\n",
       "      <td>63 pages, in German</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Text data mining and data quality management f...</td>\n",
       "      <td>In the implementation and use of research info...</td>\n",
       "      <td>Otmane Azeroual, Gunter Saake, Mohammad Abuosb...</td>\n",
       "      <td>http://arxiv.org/abs/1812.04298v1</td>\n",
       "      <td>2018-12-11T09:39:55Z</td>\n",
       "      <td>2018-12-11T09:39:55Z</td>\n",
       "      <td>cs.DL</td>\n",
       "      <td>ICOA 2018 3e colloque international sur le lib...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>A Semi-Supervised Deep Clustering Pipeline for...</td>\n",
       "      <td>Mining the latent intentions from large volume...</td>\n",
       "      <td>Xinyu Chen, Ian Beaver</td>\n",
       "      <td>http://arxiv.org/abs/2202.00802v1</td>\n",
       "      <td>2022-02-01T23:01:05Z</td>\n",
       "      <td>2022-02-01T23:01:05Z</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>None</td>\n",
       "      <td>Submitted to The Thirty-Fourth Annual Conferen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>A Case Study in Text Mining: Interpreting Twit...</td>\n",
       "      <td>Cluster analysis is a field of data analysis t...</td>\n",
       "      <td>Daniel Godfrey, Caley Johns, Carl Meyer, Shain...</td>\n",
       "      <td>http://arxiv.org/abs/1408.5427v1</td>\n",
       "      <td>2014-08-21T17:58:33Z</td>\n",
       "      <td>2014-08-21T17:58:33Z</td>\n",
       "      <td>stat.ML</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Analyses of Multi-collection Corpora via Compo...</td>\n",
       "      <td>As electronically stored data grow in daily li...</td>\n",
       "      <td>Clint P. George, Wei Xia, George Michailidis</td>\n",
       "      <td>http://arxiv.org/abs/1907.01636v1</td>\n",
       "      <td>2019-06-17T06:59:25Z</td>\n",
       "      <td>2019-06-17T06:59:25Z</td>\n",
       "      <td>cs.IR</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Conceptualized Representation Learning for Chi...</td>\n",
       "      <td>Biomedical text mining is becoming increasingl...</td>\n",
       "      <td>Ningyu Zhang, Qianghuai Jia, Kangping Yin, Lia...</td>\n",
       "      <td>http://arxiv.org/abs/2008.10813v1</td>\n",
       "      <td>2020-08-25T04:41:35Z</td>\n",
       "      <td>2020-08-25T04:41:35Z</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>None</td>\n",
       "      <td>WSDM2020 Health Day</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  \\\n",
       "0    Data, text and web mining for business intelli...   \n",
       "1    Text Data Mining from the Author's Perspective...   \n",
       "2    Very Large Language Model as a Unified Methodo...   \n",
       "3                   Pbm: A new dataset for blog mining   \n",
       "4                     Typesafe Modeling in Text Mining   \n",
       "..                                                 ...   \n",
       "245  Text data mining and data quality management f...   \n",
       "246  A Semi-Supervised Deep Clustering Pipeline for...   \n",
       "247  A Case Study in Text Mining: Interpreting Twit...   \n",
       "248  Analyses of Multi-collection Corpora via Compo...   \n",
       "249  Conceptualized Representation Learning for Chi...   \n",
       "\n",
       "                                               Summary  \\\n",
       "0    The Information and Communication Technologies...   \n",
       "1    Given the many technical, social, and policy s...   \n",
       "2    Text data mining is the process of deriving es...   \n",
       "3    Text mining is becoming vital as Web 2.0 offer...   \n",
       "4    Based on the concept of annotation-based agent...   \n",
       "..                                                 ...   \n",
       "245  In the implementation and use of research info...   \n",
       "246  Mining the latent intentions from large volume...   \n",
       "247  Cluster analysis is a field of data analysis t...   \n",
       "248  As electronically stored data grow in daily li...   \n",
       "249  Biomedical text mining is becoming increasingl...   \n",
       "\n",
       "                                               Authors  \\\n",
       "0                            Abdul-Aziz Rashid Al-Azmi   \n",
       "1                                 Christine L. Borgman   \n",
       "2                                           Meng Jiang   \n",
       "3                          Mehwish Aziz, Muhammad Rafi   \n",
       "4                                         Fabian Steeg   \n",
       "..                                                 ...   \n",
       "245  Otmane Azeroual, Gunter Saake, Mohammad Abuosb...   \n",
       "246                             Xinyu Chen, Ian Beaver   \n",
       "247  Daniel Godfrey, Caley Johns, Carl Meyer, Shain...   \n",
       "248       Clint P. George, Wei Xia, George Michailidis   \n",
       "249  Ningyu Zhang, Qianghuai Jia, Kangping Yin, Lia...   \n",
       "\n",
       "                                  Link             Published  \\\n",
       "0     http://arxiv.org/abs/1304.3563v1  2013-04-12T08:04:31Z   \n",
       "1    http://arxiv.org/abs/1803.04552v1  2018-03-12T22:11:40Z   \n",
       "2    http://arxiv.org/abs/2212.09271v2  2022-12-19T06:52:13Z   \n",
       "3     http://arxiv.org/abs/1201.2073v1  2012-01-10T15:18:38Z   \n",
       "4     http://arxiv.org/abs/1108.0363v1  2011-07-28T17:46:20Z   \n",
       "..                                 ...                   ...   \n",
       "245  http://arxiv.org/abs/1812.04298v1  2018-12-11T09:39:55Z   \n",
       "246  http://arxiv.org/abs/2202.00802v1  2022-02-01T23:01:05Z   \n",
       "247   http://arxiv.org/abs/1408.5427v1  2014-08-21T17:58:33Z   \n",
       "248  http://arxiv.org/abs/1907.01636v1  2019-06-17T06:59:25Z   \n",
       "249  http://arxiv.org/abs/2008.10813v1  2020-08-25T04:41:35Z   \n",
       "\n",
       "                  Updated Primary_Category  \\\n",
       "0    2013-04-12T08:04:31Z            cs.IR   \n",
       "1    2018-03-12T22:11:40Z            cs.DL   \n",
       "2    2022-12-20T17:03:30Z            cs.DB   \n",
       "3    2012-01-10T15:18:38Z            cs.AI   \n",
       "4    2011-07-28T17:46:20Z            cs.PL   \n",
       "..                    ...              ...   \n",
       "245  2018-12-11T09:39:55Z            cs.DL   \n",
       "246  2022-02-01T23:01:05Z            cs.CL   \n",
       "247  2014-08-21T17:58:33Z          stat.ML   \n",
       "248  2019-06-17T06:59:25Z            cs.IR   \n",
       "249  2020-08-25T04:41:35Z            cs.CL   \n",
       "\n",
       "                                     Journal_Reference  \\\n",
       "0    International Journal of Data Mining & Knowled...   \n",
       "1                                                 None   \n",
       "2                                                 None   \n",
       "3                                                 None   \n",
       "4                                                 None   \n",
       "..                                                 ...   \n",
       "245  ICOA 2018 3e colloque international sur le lib...   \n",
       "246                                               None   \n",
       "247                                               None   \n",
       "248                                               None   \n",
       "249                                               None   \n",
       "\n",
       "                                               Comment  \n",
       "0                               21 page, journal paper  \n",
       "1    Forum Statement: Data Mining with Limited Acce...  \n",
       "2                                   4 pages, 3 figures  \n",
       "3    6; Internet and Web Engineering from: Internat...  \n",
       "4                                  63 pages, in German  \n",
       "..                                                 ...  \n",
       "245                                               None  \n",
       "246  Submitted to The Thirty-Fourth Annual Conferen...  \n",
       "247                                               None  \n",
       "248                                               None  \n",
       "249                                WSDM2020 Health Day  \n",
       "\n",
       "[250 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_summary = generate_summary_ollama(articles, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here's a concise and coherent summary that combines the key ideas from the five article summaries:\\n\\nThe rapid growth of digital data has driven the development of text data mining, a process of deriving essential information from language text. Various techniques, including data mining, text mining, and web mining, are used to uncover hidden knowledge in large databases or the Internet. Text mining involves tasks such as text categorization, clustering, topic modeling, information extraction, and summarization.\\n\\nAs the Web 2.0 era has brought collaborative content creation and sharing, researchers have increasingly focused on text mining methods for discovering knowledge. A typical text mining application involves preprocessing, stemming and lemmatization, tagging and annotation, deriving knowledge patterns, evaluating, and interpreting results. Standard datasets are crucial for evaluating these tasks, but there is a growing need to standardize the evaluation of text mining tasks.\\n\\nA unified methodology, such as very large language models (VLLMs), has been proposed as an effective approach to text mining. VLLMs can offer advantages over conventional methods in terms of efficiency and scalability. However, designing and developing VLLM techniques for text mining also presents challenges.\\n\\nThe increasing importance of text mining is evident in the growing interest in blog-mining, which involves analyzing blogs, a new genre in web 2.0 that offers rich knowledge sources but also poses unique challenges. The development of standardized datasets, such as the Pakistani Political Blogosphere dataset, can facilitate research and innovation in this domain.\\n\\nFinally, a formal notation and tools have been introduced for defining and running text mining experiments using a statically typed domain-specific language embedded in Scala. This framework enables researchers to develop and document text mining experiments and demonstrate how generic, typesafe annotation corresponds to a general information model that goes beyond text processing.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keyword</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15337</th>\n",
       "      <td>natural language processing</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2937</th>\n",
       "      <td>biomedical text mining</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23590</th>\n",
       "      <td>text mining techniques</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23533</th>\n",
       "      <td>text mining methods</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23587</th>\n",
       "      <td>text mining tasks</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23294</th>\n",
       "      <td>text data mining</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17605</th>\n",
       "      <td>pre trained language</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2171</th>\n",
       "      <td>association rule mining</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10308</th>\n",
       "      <td>https github com</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23990</th>\n",
       "      <td>time series data</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12086</th>\n",
       "      <td>language processing nlp</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23534</th>\n",
       "      <td>text mining models</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18865</th>\n",
       "      <td>rare temporal patterns</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12169</th>\n",
       "      <td>large language models</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15340</th>\n",
       "      <td>natural language text</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25248</th>\n",
       "      <td>using text mining</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23052</th>\n",
       "      <td>temporal pattern mining</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21817</th>\n",
       "      <td>state art methods</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3345</th>\n",
       "      <td>causal text mining</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24272</th>\n",
       "      <td>trained language models</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14311</th>\n",
       "      <td>mining natural language</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5580</th>\n",
       "      <td>data mining techniques</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10572</th>\n",
       "      <td>image text matching</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8226</th>\n",
       "      <td>experimental results show</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20846</th>\n",
       "      <td>series data mining</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23447</th>\n",
       "      <td>text mining approaches</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14477</th>\n",
       "      <td>mining sentiment analysis</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21421</th>\n",
       "      <td>social opinion mining</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17482</th>\n",
       "      <td>positive negative neutral</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2424</th>\n",
       "      <td>available https github</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23444</th>\n",
       "      <td>text mining applications</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20754</th>\n",
       "      <td>sentiment analysis opinion</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158</th>\n",
       "      <td>analysis opinion mining</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4236</th>\n",
       "      <td>comparative opinion mining</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1559</th>\n",
       "      <td>applying text mining</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23598</th>\n",
       "      <td>text mining tools</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12928</th>\n",
       "      <td>machine learning techniques</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14615</th>\n",
       "      <td>mining text mining</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20653</th>\n",
       "      <td>semantic web mining</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21407</th>\n",
       "      <td>social media platforms</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22014</th>\n",
       "      <td>structured data text</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>hard negative mining</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23440</th>\n",
       "      <td>text mining analysis</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12046</th>\n",
       "      <td>language models llms</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24271</th>\n",
       "      <td>trained language model</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20827</th>\n",
       "      <td>sequential pattern mining</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25696</th>\n",
       "      <td>web content mining</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19125</th>\n",
       "      <td>referring image segmentation</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20162</th>\n",
       "      <td>rule text mining</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20387</th>\n",
       "      <td>scientific technological papers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Keyword  Count\n",
       "15337      natural language processing     39\n",
       "2937            biomedical text mining     20\n",
       "23590           text mining techniques     20\n",
       "23533              text mining methods     16\n",
       "23587                text mining tasks     14\n",
       "23294                 text data mining     12\n",
       "17605             pre trained language     12\n",
       "2171           association rule mining     11\n",
       "10308                 https github com      9\n",
       "23990                 time series data      9\n",
       "12086          language processing nlp      9\n",
       "23534               text mining models      9\n",
       "18865           rare temporal patterns      8\n",
       "12169            large language models      8\n",
       "15340            natural language text      8\n",
       "25248                using text mining      8\n",
       "23052          temporal pattern mining      8\n",
       "21817                state art methods      8\n",
       "3345                causal text mining      7\n",
       "24272          trained language models      7\n",
       "14311          mining natural language      7\n",
       "5580            data mining techniques      7\n",
       "10572              image text matching      7\n",
       "8226         experimental results show      7\n",
       "20846               series data mining      7\n",
       "23447           text mining approaches      7\n",
       "14477        mining sentiment analysis      6\n",
       "21421            social opinion mining      6\n",
       "17482        positive negative neutral      6\n",
       "2424            available https github      6\n",
       "23444         text mining applications      6\n",
       "20754       sentiment analysis opinion      6\n",
       "1158           analysis opinion mining      6\n",
       "4236        comparative opinion mining      6\n",
       "1559              applying text mining      6\n",
       "23598                text mining tools      6\n",
       "12928      machine learning techniques      6\n",
       "14615               mining text mining      6\n",
       "20653              semantic web mining      5\n",
       "21407           social media platforms      5\n",
       "22014             structured data text      5\n",
       "9974              hard negative mining      5\n",
       "23440             text mining analysis      5\n",
       "12046             language models llms      5\n",
       "24271           trained language model      5\n",
       "20827        sequential pattern mining      5\n",
       "25696               web content mining      5\n",
       "19125     referring image segmentation      4\n",
       "20162                 rule text mining      4\n",
       "20387  scientific technological papers      4"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Similarity_Score</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Very Large Language Model as a Unified Methodo...</td>\n",
       "      <td>0.218881</td>\n",
       "      <td>http://arxiv.org/abs/2212.09271v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Multi-Task Learning Improves Performance In De...</td>\n",
       "      <td>0.169973</td>\n",
       "      <td>http://arxiv.org/abs/2307.01401v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>Semantic Web Requirements through Web Mining T...</td>\n",
       "      <td>0.160314</td>\n",
       "      <td>http://arxiv.org/abs/1208.0690v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Probabilistic Semantic Web Mining Using Artifi...</td>\n",
       "      <td>0.155123</td>\n",
       "      <td>http://arxiv.org/abs/1004.1794v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data, text and web mining for business intelli...</td>\n",
       "      <td>0.152596</td>\n",
       "      <td>http://arxiv.org/abs/1304.3563v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Overview of Web Content Mining Tools</td>\n",
       "      <td>0.149120</td>\n",
       "      <td>http://arxiv.org/abs/1307.1024v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Advancing Chinese biomedical text mining with ...</td>\n",
       "      <td>0.146190</td>\n",
       "      <td>http://arxiv.org/abs/2403.04261v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Opinion Mining In Hindi Language: A Survey</td>\n",
       "      <td>0.135362</td>\n",
       "      <td>http://arxiv.org/abs/1404.4935v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sentiment Analysis: A Survey</td>\n",
       "      <td>0.134661</td>\n",
       "      <td>http://arxiv.org/abs/1405.2584v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Meta-learning of textual representations</td>\n",
       "      <td>0.132407</td>\n",
       "      <td>http://arxiv.org/abs/1906.08934v2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Title  Similarity_Score  \\\n",
       "2    Very Large Language Model as a Unified Methodo...          0.218881   \n",
       "38   Multi-Task Learning Improves Performance In De...          0.169973   \n",
       "174  Semantic Web Requirements through Web Mining T...          0.160314   \n",
       "70   Probabilistic Semantic Web Mining Using Artifi...          0.155123   \n",
       "0    Data, text and web mining for business intelli...          0.152596   \n",
       "42                Overview of Web Content Mining Tools          0.149120   \n",
       "46   Advancing Chinese biomedical text mining with ...          0.146190   \n",
       "146         Opinion Mining In Hindi Language: A Survey          0.135362   \n",
       "14                        Sentiment Analysis: A Survey          0.134661   \n",
       "92            Meta-learning of textual representations          0.132407   \n",
       "\n",
       "                                  Link  \n",
       "2    http://arxiv.org/abs/2212.09271v2  \n",
       "38   http://arxiv.org/abs/2307.01401v1  \n",
       "174   http://arxiv.org/abs/1208.0690v1  \n",
       "70    http://arxiv.org/abs/1004.1794v1  \n",
       "0     http://arxiv.org/abs/1304.3563v1  \n",
       "42    http://arxiv.org/abs/1307.1024v1  \n",
       "46   http://arxiv.org/abs/2403.04261v2  \n",
       "146   http://arxiv.org/abs/1404.4935v1  \n",
       "14    http://arxiv.org/abs/1405.2584v1  \n",
       "92   http://arxiv.org/abs/1906.08934v2  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_url = \"http://arxiv.org/abs/1201.2073v1\" \n",
    "similar_articles_with_urls = find_similar_articles_with_url(articles, article_url, top_n=10)\n",
    "similar_articles_with_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
