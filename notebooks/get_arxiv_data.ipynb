{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\joaop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções\n",
    "\n",
    "def get_arxiv_data(query, max_results=250, sort_by=\"relevance\"):\n",
    "    \"\"\"\n",
    "    Obtém artigos da arXiv API com base numa query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Palavra-chave ou expressão de pesquisa.\n",
    "        max_results (int): Número máximo de resultados a retornar\n",
    "        sort_by (str): Critério de ordenação ('relevance', 'submittedDate', 'lastUpdatedDate').\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: Dados dos artigos recolhidos.\n",
    "    \"\"\"\n",
    "    base_url = \"http://export.arxiv.org/api/query\"\n",
    "    results = []\n",
    "\n",
    "    # Dividir as pesquisas em lotes de 1000 (limitação da arXiv API)\n",
    "    for start in range(0, max_results, 1000):\n",
    "        params = {\n",
    "            \"search_query\": query,\n",
    "            \"start\": start,\n",
    "            \"max_results\": min(max_results - start, 1000),\n",
    "            \"sortBy\": sort_by,  # Critério de ordenação\n",
    "        }\n",
    "\n",
    "        # Requisição à arXiv API\n",
    "        response = requests.get(base_url, params=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            # Parsear o XML retornado\n",
    "            root = ET.fromstring(response.text)\n",
    "            for entry in root.findall(\"{http://www.w3.org/2005/Atom}entry\"):\n",
    "                # Extrair campos importantes do artigo\n",
    "                title = entry.find(\"{http://www.w3.org/2005/Atom}title\").text.strip()\n",
    "                summary = entry.find(\"{http://www.w3.org/2005/Atom}summary\").text.strip()\n",
    "                link = entry.find(\"{http://www.w3.org/2005/Atom}id\").text.strip()\n",
    "                published = entry.find(\"{http://www.w3.org/2005/Atom}published\").text.strip()\n",
    "                updated = entry.find(\"{http://www.w3.org/2005/Atom}updated\").text.strip()\n",
    "                authors = [\n",
    "                    author.find(\"{http://www.w3.org/2005/Atom}name\").text.strip()\n",
    "                    for author in entry.findall(\"{http://www.w3.org/2005/Atom}author\")\n",
    "                ]\n",
    "\n",
    "                # Adicionar aos resultados\n",
    "                results.append({\n",
    "                    \"Title\": title,\n",
    "                    \"Summary\": summary,\n",
    "                    \"Authors\": \", \".join(authors),\n",
    "                    \"Link\": link,\n",
    "                    \"Published\": published,\n",
    "                    \"Updated\": updated,\n",
    "                })\n",
    "        else:\n",
    "            print(f\"Erro na API: {response.status_code}\")\n",
    "            break\n",
    "\n",
    "    # Converter para DataFrame\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def extract_keywords(df, top_n=10):\n",
    "    \"\"\"\n",
    "    Gera três DataFrames separados para palavras individuais, bigramas e trigramas mais frequentes.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): DataFrame com os artigos.\n",
    "        top_n (int): Número de palavras-chave mais frequentes a retornar para cada n-grama.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: DataFrames para unigramas, bigramas e trigramas.\n",
    "    \"\"\"\n",
    "    # Concatenar títulos e resumos em um único texto\n",
    "    combined_text = \" \".join(df[\"Title\"]) + \" \" + \" \".join(df[\"Summary\"])\n",
    "    \n",
    "    # Remover stopwords\n",
    "    stop_words = list(stopwords.words(\"english\"))\n",
    "\n",
    "    # Criar vetorizadores para unigramas, bigramas e trigramas\n",
    "    vectorizer_uni = CountVectorizer(ngram_range=(1, 1), stop_words=stop_words)\n",
    "    vectorizer_bi = CountVectorizer(ngram_range=(2, 2), stop_words=stop_words)\n",
    "    vectorizer_tri = CountVectorizer(ngram_range=(3, 3), stop_words=stop_words)\n",
    "    \n",
    "    # Criar vetores de palavras\n",
    "    word_counts_uni = vectorizer_uni.fit_transform([combined_text])\n",
    "    word_counts_bi = vectorizer_bi.fit_transform([combined_text])\n",
    "    word_counts_tri = vectorizer_tri.fit_transform([combined_text])\n",
    "    \n",
    "    # Criar DataFrames para unigramas\n",
    "    word_list_uni = vectorizer_uni.get_feature_names_out()\n",
    "    count_list_uni = word_counts_uni.toarray().flatten()\n",
    "    unigrams_df = pd.DataFrame({\"Keyword\": word_list_uni, \"Count\": count_list_uni})\n",
    "    unigrams_df = unigrams_df.sort_values(by=\"Count\", ascending=False).head(top_n)\n",
    "    \n",
    "    # Criar DataFrames para bigramas\n",
    "    word_list_bi = vectorizer_bi.get_feature_names_out()\n",
    "    count_list_bi = word_counts_bi.toarray().flatten()\n",
    "    bigrams_df = pd.DataFrame({\"Keyword\": word_list_bi, \"Count\": count_list_bi})\n",
    "    bigrams_df = bigrams_df.sort_values(by=\"Count\", ascending=False).head(top_n)\n",
    "    \n",
    "    # Criar DataFrames para trigramas\n",
    "    word_list_tri = vectorizer_tri.get_feature_names_out()\n",
    "    count_list_tri = word_counts_tri.toarray().flatten()\n",
    "    trigrams_df = pd.DataFrame({\"Keyword\": word_list_tri, \"Count\": count_list_tri})\n",
    "    trigrams_df = trigrams_df.sort_values(by=\"Count\", ascending=False).head(top_n)\n",
    "    \n",
    "    return unigrams_df, bigrams_df, trigrams_df\n",
    "\n",
    "\n",
    "def filter_articles_by_keywords(df, keywords):\n",
    "    \"\"\"\n",
    "    Filtra artigos que contêm palavras-chave relevantes no título ou resumo. \n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame com os artigos.\n",
    "        keywords (list): Lista de palavras-chave relevantes (tem de estar tudo em lower-case).\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Apenas os artigos relevantes.\n",
    "    \"\"\"\n",
    "    def is_relevant(row):\n",
    "        text = (row[\"Title\"] + \" \" + row[\"Summary\"]).lower()\n",
    "        return any(keyword in text for keyword in keywords)\n",
    "    \n",
    "    df[\"Is_Relevant\"] = df.apply(is_relevant, axis=1)\n",
    "    return df[df[\"Is_Relevant\"]]\n",
    "\n",
    "\n",
    "def analyze_publication_trends(df):\n",
    "    \"\"\"\n",
    "    Analisa a frequência de publicações ao longo do tempo.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame com os artigos.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Frequência de publicações por ano.\n",
    "    \"\"\"\n",
    "    df[\"Published_Year\"] = pd.to_datetime(df[\"Published\"]).dt.year\n",
    "    trends = df.groupby(\"Published_Year\").size().reset_index(name=\"Publication_Count\")\n",
    "    return trends\n",
    "\n",
    "\n",
    "def top_authors(df, top_n=10):\n",
    "    \"\"\"\n",
    "    Identifica os autores com mais publicações.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame com os artigos.\n",
    "        top_n (int): Número de autores a listar.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Autores mais frequentes e o número de publicações.\n",
    "    \"\"\"\n",
    "    authors_series = df[\"Authors\"].str.split(\", \").explode()\n",
    "    author_counts = authors_series.value_counts().head(top_n).reset_index()\n",
    "    author_counts.columns = [\"Author\", \"Publication_Count\"]\n",
    "    return author_counts\n",
    "\n",
    "\n",
    "def cluster_articles(df, n_clusters=5):\n",
    "    \"\"\"\n",
    "    Agrupa os artigos em clusters temáticos.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame com os artigos.\n",
    "        n_clusters (int): Número de clusters.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame com uma nova coluna \"Cluster\".\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[\"Summary\"])\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    df[\"Cluster\"] = kmeans.fit_predict(tfidf_matrix)\n",
    "    return df\n",
    "\n",
    "\n",
    "def find_similar_articles_with_url(df, article_url, top_n=5):\n",
    "    \"\"\"\n",
    "    Identifica os artigos mais similares a um artigo específico com base na URL e mostra as URLs nos resultados.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame com os artigos.\n",
    "        article_url (str): URL do artigo para o qual encontrar similares.\n",
    "        top_n (int): Número de artigos similares a retornar.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame com os artigos similares, suas pontuações de similaridade e URLs.\n",
    "    \"\"\"\n",
    "    # Verificar se a URL existe no DataFrame\n",
    "    if article_url not in df[\"Link\"].values:\n",
    "        raise ValueError(\"A URL fornecida não está no conjunto de dados.\")\n",
    "    \n",
    "    # Obter o índice do artigo de interesse\n",
    "    article_index = df[df[\"Link\"] == article_url].index[0]\n",
    "\n",
    "    # Vetorização TF-IDF\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = vectorizer.fit_transform(df[\"Summary\"])\n",
    "    \n",
    "    # Similaridade do artigo selecionado com todos os outros\n",
    "    similarity_scores = cosine_similarity(tfidf_matrix[article_index], tfidf_matrix).flatten()\n",
    "    \n",
    "    # Ordenar pelos artigos mais similares (excluindo o próprio)\n",
    "    similar_indices = similarity_scores.argsort()[-top_n-1:-1][::-1]\n",
    "    similar_scores = similarity_scores[similar_indices]\n",
    "    \n",
    "    # Criar DataFrame com resultados\n",
    "    similar_articles = df.iloc[similar_indices].copy()\n",
    "    similar_articles[\"Similarity_Score\"] = similar_scores\n",
    "    return similar_articles[[\"Title\", \"Similarity_Score\", \"Link\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"reinforcement learning\"  # Palavra-chave para pesquisas\n",
    "max_results = 5000  # Quantidade máxima de artigos\n",
    "\n",
    "articles = get_arxiv_data(query, max_results, sort_by='relevance')\n",
    "unigrams, bigrams, trigrams = extract_keywords(articles, top_n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Link</th>\n",
       "      <th>Published</th>\n",
       "      <th>Updated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Some Insights into Lifelong Reinforcement Lear...</td>\n",
       "      <td>A lifelong reinforcement learning system is a ...</td>\n",
       "      <td>Changjian Li</td>\n",
       "      <td>http://arxiv.org/abs/2001.09608v1</td>\n",
       "      <td>2020-01-27T07:26:12Z</td>\n",
       "      <td>2020-01-27T07:26:12Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Counterexample-Guided Repair of Reinforcement ...</td>\n",
       "      <td>Naively trained Deep Reinforcement Learning ag...</td>\n",
       "      <td>David Boetius, Stefan Leue</td>\n",
       "      <td>http://arxiv.org/abs/2405.15430v1</td>\n",
       "      <td>2024-05-24T10:56:51Z</td>\n",
       "      <td>2024-05-24T10:56:51Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Deep Reinforcement Learning in Computer Vision...</td>\n",
       "      <td>Deep reinforcement learning augments the reinf...</td>\n",
       "      <td>Ngan Le, Vidhiwar Singh Rathour, Kashu Yamazak...</td>\n",
       "      <td>http://arxiv.org/abs/2108.11510v1</td>\n",
       "      <td>2021-08-25T23:01:48Z</td>\n",
       "      <td>2021-08-25T23:01:48Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Causal Reinforcement Learning: A Survey</td>\n",
       "      <td>Reinforcement learning is an essential paradig...</td>\n",
       "      <td>Zhihong Deng, Jing Jiang, Guodong Long, Chengq...</td>\n",
       "      <td>http://arxiv.org/abs/2307.01452v2</td>\n",
       "      <td>2023-07-04T03:00:43Z</td>\n",
       "      <td>2023-11-21T03:43:15Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Distributed Deep Reinforcement Learning: A Sur...</td>\n",
       "      <td>With the breakthrough of AlphaGo, deep reinfor...</td>\n",
       "      <td>Qiyue Yin, Tongtong Yu, Shengqi Shen, Jun Yang...</td>\n",
       "      <td>http://arxiv.org/abs/2212.00253v1</td>\n",
       "      <td>2022-12-01T03:39:24Z</td>\n",
       "      <td>2022-12-01T03:39:24Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Some Insights into Lifelong Reinforcement Lear...   \n",
       "1  Counterexample-Guided Repair of Reinforcement ...   \n",
       "2  Deep Reinforcement Learning in Computer Vision...   \n",
       "3            Causal Reinforcement Learning: A Survey   \n",
       "4  Distributed Deep Reinforcement Learning: A Sur...   \n",
       "\n",
       "                                             Summary  \\\n",
       "0  A lifelong reinforcement learning system is a ...   \n",
       "1  Naively trained Deep Reinforcement Learning ag...   \n",
       "2  Deep reinforcement learning augments the reinf...   \n",
       "3  Reinforcement learning is an essential paradig...   \n",
       "4  With the breakthrough of AlphaGo, deep reinfor...   \n",
       "\n",
       "                                             Authors  \\\n",
       "0                                       Changjian Li   \n",
       "1                         David Boetius, Stefan Leue   \n",
       "2  Ngan Le, Vidhiwar Singh Rathour, Kashu Yamazak...   \n",
       "3  Zhihong Deng, Jing Jiang, Guodong Long, Chengq...   \n",
       "4  Qiyue Yin, Tongtong Yu, Shengqi Shen, Jun Yang...   \n",
       "\n",
       "                                Link             Published  \\\n",
       "0  http://arxiv.org/abs/2001.09608v1  2020-01-27T07:26:12Z   \n",
       "1  http://arxiv.org/abs/2405.15430v1  2024-05-24T10:56:51Z   \n",
       "2  http://arxiv.org/abs/2108.11510v1  2021-08-25T23:01:48Z   \n",
       "3  http://arxiv.org/abs/2307.01452v2  2023-07-04T03:00:43Z   \n",
       "4  http://arxiv.org/abs/2212.00253v1  2022-12-01T03:39:24Z   \n",
       "\n",
       "                Updated  \n",
       "0  2020-01-27T07:26:12Z  \n",
       "1  2024-05-24T10:56:51Z  \n",
       "2  2021-08-25T23:01:48Z  \n",
       "3  2023-11-21T03:43:15Z  \n",
       "4  2022-12-01T03:39:24Z  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Similarity_Score</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1283</th>\n",
       "      <td>Deep Reinforcement Learning with Enhanced Safe...</td>\n",
       "      <td>0.221889</td>\n",
       "      <td>http://arxiv.org/abs/1910.12905v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>935</th>\n",
       "      <td>Context-Aware Safe Reinforcement Learning for ...</td>\n",
       "      <td>0.216600</td>\n",
       "      <td>http://arxiv.org/abs/2101.00531v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2541</th>\n",
       "      <td>A Safety Modulator Actor-Critic Method in Mode...</td>\n",
       "      <td>0.216551</td>\n",
       "      <td>http://arxiv.org/abs/2410.06847v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2880</th>\n",
       "      <td>SCPO: Safe Reinforcement Learning with Safety ...</td>\n",
       "      <td>0.214891</td>\n",
       "      <td>http://arxiv.org/abs/2311.00880v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>Safe Reinforcement Learning by Imagining the N...</td>\n",
       "      <td>0.197567</td>\n",
       "      <td>http://arxiv.org/abs/2202.07789v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>Learning-based Model Predictive Control for Sa...</td>\n",
       "      <td>0.196762</td>\n",
       "      <td>http://arxiv.org/abs/1906.12189v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1168</th>\n",
       "      <td>Improving Safety in Deep Reinforcement Learnin...</td>\n",
       "      <td>0.192122</td>\n",
       "      <td>http://arxiv.org/abs/2109.14325v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>Weakly Supervised Reinforcement Learning for A...</td>\n",
       "      <td>0.187187</td>\n",
       "      <td>http://arxiv.org/abs/2103.09726v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>Verifiably Safe Off-Model Reinforcement Learning</td>\n",
       "      <td>0.180749</td>\n",
       "      <td>http://arxiv.org/abs/1902.05632v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3555</th>\n",
       "      <td>MAMPS: Safe Multi-Agent Reinforcement Learning...</td>\n",
       "      <td>0.177638</td>\n",
       "      <td>http://arxiv.org/abs/1910.12639v2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Title  Similarity_Score  \\\n",
       "1283  Deep Reinforcement Learning with Enhanced Safe...          0.221889   \n",
       "935   Context-Aware Safe Reinforcement Learning for ...          0.216600   \n",
       "2541  A Safety Modulator Actor-Critic Method in Mode...          0.216551   \n",
       "2880  SCPO: Safe Reinforcement Learning with Safety ...          0.214891   \n",
       "534   Safe Reinforcement Learning by Imagining the N...          0.197567   \n",
       "592   Learning-based Model Predictive Control for Sa...          0.196762   \n",
       "1168  Improving Safety in Deep Reinforcement Learnin...          0.192122   \n",
       "1250  Weakly Supervised Reinforcement Learning for A...          0.187187   \n",
       "1501   Verifiably Safe Off-Model Reinforcement Learning          0.180749   \n",
       "3555  MAMPS: Safe Multi-Agent Reinforcement Learning...          0.177638   \n",
       "\n",
       "                                   Link  \n",
       "1283  http://arxiv.org/abs/1910.12905v2  \n",
       "935   http://arxiv.org/abs/2101.00531v1  \n",
       "2541  http://arxiv.org/abs/2410.06847v1  \n",
       "2880  http://arxiv.org/abs/2311.00880v1  \n",
       "534   http://arxiv.org/abs/2202.07789v1  \n",
       "592   http://arxiv.org/abs/1906.12189v1  \n",
       "1168  http://arxiv.org/abs/2109.14325v1  \n",
       "1250  http://arxiv.org/abs/2103.09726v1  \n",
       "1501  http://arxiv.org/abs/1902.05632v1  \n",
       "3555  http://arxiv.org/abs/1910.12639v2  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_url = \"http://arxiv.org/abs/2405.15430v1\" \n",
    "similar_articles_with_urls = find_similar_articles_with_url(articles, article_url, top_n=10)\n",
    "similar_articles_with_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
